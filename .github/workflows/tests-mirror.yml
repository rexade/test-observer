name: Tests & Mirror (Pytest)

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

permissions:
  contents: read
  actions: read
  pull-requests: write

jobs:
  test-and-mirror:
    runs-on: ubuntu-latest
    env:
      PYTHONDONTWRITEBYTECODE: 1
      PIP_DISABLE_PIP_VERSION_CHECK: 1
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.MIRROR_SERVICE_KEY }}
      PROJECT_SLUG: ${{ github.repository }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install deps (pytest + coverage + lxml)
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-cov requests lxml

      - name: Run tests
        run: |
          mkdir -p reports
          pytest -q \
            --maxfail=1 \
            --cov=. --cov-report=xml:reports/coverage.xml \
            --junitxml=reports/junit.xml \
            -o junit_family=xunit2
        continue-on-error: true

      - name: Generate signed manifest
        run: |
          python scripts/generate_manifest.py reports reports/manifest.json
        continue-on-error: true

      - name: Detect flaky tests
        run: |
          python scripts/detect_flaky.py reports/junit.xml
        continue-on-error: true

      - name: Upload artifacts to GitHub
        uses: actions/upload-artifact@v4
        with:
          name: test-artifacts
          path: reports/

      - name: Upload artifacts to Supabase Storage
        run: |
          RUN_ID="${GITHUB_RUN_ID}-${GITHUB_RUN_ATTEMPT}"
          
          for file in reports/*.xml reports/*.json; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              echo "Uploading ${filename}..."
              response=$(curl -sS -X POST \
                "${SUPABASE_URL}/storage/v1/object/test-artifacts/runs/${RUN_ID}/${filename}" \
                -H "Authorization: Bearer ${SUPABASE_SERVICE_ROLE_KEY}" \
                -H "apikey: ${SUPABASE_ANON_KEY}" \
                -H "Content-Type: application/octet-stream" \
                --data-binary "@${file}" \
                -w "\n%{http_code}")
              echo "Response: $response"
            fi
          done

      - name: Compute coverage quadrants & prepare payload
        id: payload
        shell: python
        run: |
          import json, os, pathlib
          import xml.etree.ElementTree as ET
          
          run_id = f"{os.environ['GITHUB_RUN_ID']}-{os.environ.get('GITHUB_RUN_ATTEMPT','1')}"
          artifact_base = f"{os.environ['SUPABASE_URL']}/storage/v1/object/public/test-artifacts/runs/{run_id}"
          
          # Load requirements registry
          requirements_map = {}
          if os.path.exists('tests/fixtures/requirements.json'):
            requirements_map = json.loads(open('tests/fixtures/requirements.json').read())
          
          # Parse junit.xml for test outcomes
          try:
            junit = ET.parse('reports/junit.xml').getroot()
            total = sum(int(s.get('tests', 0)) for s in junit.iter('testsuite'))
            failures = sum(int(s.get('failures', 0)) for s in junit.iter('testsuite'))
            errors = sum(int(s.get('errors', 0)) for s in junit.iter('testsuite'))
            passed = max(total - failures - errors, 0)
            
            # Compute coverage quadrants
            requirement = passed / total if total else 0
            
            # Check for temporal tests (timing/latency keywords or @temporal marker)
            temporal_tests = [c for c in junit.iter('testcase') 
                            if 'timing' in (c.get('name','')+c.get('classname','')).lower() 
                            or 'latency' in (c.get('name','')+c.get('classname','')).lower()
                            or 'temporal' in (c.get('name','')+c.get('classname','')).lower()
                            or 'rfc5905' in (c.get('name','')+c.get('classname','')).lower()]
            temporal = len(temporal_tests) / total if total else 0.2
            
            # Check for interface/contract tests (schema/XSD validation)
            interface_tests = [c for c in junit.iter('testcase')
                             if 'interface' in (c.get('name','')+c.get('classname','')).lower()
                             or 'contract' in (c.get('name','')+c.get('classname','')).lower()
                             or 'schema' in (c.get('name','')+c.get('classname','')).lower()
                             or 'xsd' in (c.get('name','')+c.get('classname','')).lower()]
            interface = len(interface_tests) / total if total else 0.3
            
            # Risk: weight critical requirements higher
            critical_reqs = sum(1 for r in requirements_map.values() if r.get('risk') == 'critical')
            risk_score = 0.5
            if critical_reqs > 0:
              # Higher risk score if critical requirements are covered
              critical_passed = sum(1 for c in junit.iter('testcase') 
                                  if c.find('failure') is None and c.find('error') is None)
              risk_score = min(1.0, 0.3 + (0.7 * critical_passed / total)) if total else 0.5
            risk = risk_score
            
          except Exception as e:
            print(f"Warning: Could not parse junit.xml: {e}")
            requirement, temporal, interface, risk = 0.55, 0.40, 0.70, 0.50
          
          # Build decisions from test results
          decisions = []
          flaky_tests = []
          
          # Load flaky test data if available
          try:
            if os.path.exists('reports/flaky_tests.json'):
              flaky_data = json.loads(open('reports/flaky_tests.json').read())
              flaky_tests = list(flaky_data.keys())
          except Exception:
            pass
          
          try:
            for testcase in junit.iter('testcase'):
              name = testcase.get('name', 'Unknown')
              classname = testcase.get('classname', '')
              full_name = f"{classname}.{name}"
              failure = testcase.find('failure')
              error = testcase.find('error')
              
              decision = {
                "oracle": full_name,
                "evidence": [f"{artifact_base}/junit.xml"]
              }
              
              # Mark flaky tests
              if full_name in flaky_tests:
                decision["message"] = "âš  Flaky test detected"
              
              if failure is not None:
                decision["result"] = "fail"
                decision["message"] = decision.get("message", "") + " | " + failure.get('message', 'Test failed')
              elif error is not None:
                decision["result"] = "error"
                decision["message"] = decision.get("message", "") + " | " + error.get('message', 'Test error')
              else:
                decision["result"] = "pass"
              
              decisions.append(decision)
          except Exception as e:
            print(f"Warning: Could not extract decisions: {e}")
            decisions = [{"oracle": "pytest", "result": "pass", "evidence": [f"{artifact_base}/junit.xml"]}]
          
          # Build payload
          payload = {
            "run": {
              "run_id": run_id,
              "project": os.environ['PROJECT_SLUG'],
              "commit": os.environ['GITHUB_SHA'][:7],
              "branch": os.environ['GITHUB_REF_NAME'],
              "created_at": os.environ.get('GITHUB_EVENT_HEAD_COMMIT_TIMESTAMP') or __import__('datetime').datetime.now(__import__('datetime').timezone.utc).isoformat(),
              "ci": {
                "provider": "github",
                "workflow": os.environ.get('GITHUB_WORKFLOW', 'Tests'),
                "run_url": f"https://github.com/{os.environ['PROJECT_SLUG']}/actions/runs/{os.environ['GITHUB_RUN_ID']}"
              }
            },
            "manifest": {
              "schema": "mirror.run-manifest.v1",
              "counts": {"events": total},
              "tooling": {"evaluator": "pytest"}
            },
            "coverage": {
              "requirement": round(requirement, 3),
              "temporal": round(temporal, 3),
              "interface": round(interface, 3),
              "risk": round(risk, 3)
            },
            "decisions": decisions
          }
          
          pathlib.Path('reports/payload.json').write_text(json.dumps(payload, indent=2))
          
          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
            f.write(f"run_id={run_id}\n")
            f.write(f"total_tests={total}\n")
            f.write(f"passed_tests={passed}\n")

      - name: Upload payload.json
        uses: actions/upload-artifact@v4
        with:
          name: mirror-payload
          path: reports/payload.json

      - name: Post to Mirror API (runs)
        run: |
          curl -sS -X POST "$SUPABASE_URL/functions/v1/runs" \
            -H "Authorization: Bearer $SUPABASE_SERVICE_ROLE_KEY" \
            -H "Content-Type: application/json" \
            -H "Idempotency-Key: ${GITHUB_RUN_ID}-${GITHUB_RUN_ATTEMPT}" \
            --data-binary @reports/payload.json

      - name: Comment coverage badge on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const project = encodeURIComponent(process.env.PROJECT_SLUG);
            const badgeUrl = `${process.env.SUPABASE_URL}/functions/v1/coverage-badge/${project}`;
            const runUrl = `https://github.com/${process.env.PROJECT_SLUG}/actions/runs/${process.env.GITHUB_RUN_ID}`;
            const body = `## ðŸ“Š Test Coverage
            
            ![Coverage](${badgeUrl})
            
            **Run**: [\`${{ steps.payload.outputs.run_id }}\`](${runUrl})
            **Tests**: ${{ steps.payload.outputs.passed_tests }}/${{ steps.payload.outputs.total_tests }} passed`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body
            });
